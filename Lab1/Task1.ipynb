{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abbas\\AppData\\Local\\Temp\\ipykernel_2028\\1370486045.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df=pd.read_csv(r'E:\\Uni\\5thSem\\ML\\Lab\\Lab1\\ECG200_TRAIN.csv',delimiter='  ')\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r'E:\\Uni\\5thSem\\ML\\Lab\\Lab1\\ECG200_TRAIN.csv',delimiter='  ')\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [str(i) for i in range (len(df.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.804668</td>\n",
       "      <td>0.367771</td>\n",
       "      <td>0.243894</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>-0.274402</td>\n",
       "      <td>0.096731</td>\n",
       "      <td>-0.747731</td>\n",
       "      <td>-1.609777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.770792</td>\n",
       "      <td>-0.533503</td>\n",
       "      <td>-0.400228</td>\n",
       "      <td>0.176084</td>\n",
       "      <td>1.111768</td>\n",
       "      <td>2.438428</td>\n",
       "      <td>2.734889</td>\n",
       "      <td>1.736054</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>-1.265074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.316646</td>\n",
       "      <td>0.243199</td>\n",
       "      <td>0.370471</td>\n",
       "      <td>1.063738</td>\n",
       "      <td>1.678187</td>\n",
       "      <td>1.759558</td>\n",
       "      <td>1.697717</td>\n",
       "      <td>1.612159</td>\n",
       "      <td>1.168188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771598</td>\n",
       "      <td>0.764229</td>\n",
       "      <td>0.610621</td>\n",
       "      <td>0.552900</td>\n",
       "      <td>0.566786</td>\n",
       "      <td>0.604002</td>\n",
       "      <td>0.777068</td>\n",
       "      <td>0.812345</td>\n",
       "      <td>0.748848</td>\n",
       "      <td>0.818042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.168874</td>\n",
       "      <td>2.075901</td>\n",
       "      <td>1.760141</td>\n",
       "      <td>1.606446</td>\n",
       "      <td>1.949046</td>\n",
       "      <td>1.302842</td>\n",
       "      <td>0.459332</td>\n",
       "      <td>0.516412</td>\n",
       "      <td>0.852180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186409</td>\n",
       "      <td>0.419006</td>\n",
       "      <td>0.723888</td>\n",
       "      <td>1.323947</td>\n",
       "      <td>2.136488</td>\n",
       "      <td>1.746597</td>\n",
       "      <td>1.470220</td>\n",
       "      <td>1.893512</td>\n",
       "      <td>1.256949</td>\n",
       "      <td>0.800407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648658</td>\n",
       "      <td>0.752026</td>\n",
       "      <td>2.636231</td>\n",
       "      <td>3.455716</td>\n",
       "      <td>2.118157</td>\n",
       "      <td>0.520620</td>\n",
       "      <td>-0.188627</td>\n",
       "      <td>0.780818</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077445</td>\n",
       "      <td>-0.097869</td>\n",
       "      <td>-0.136787</td>\n",
       "      <td>-0.340237</td>\n",
       "      <td>-0.089441</td>\n",
       "      <td>-0.080297</td>\n",
       "      <td>-0.192584</td>\n",
       "      <td>-0.304704</td>\n",
       "      <td>-0.454556</td>\n",
       "      <td>0.314590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404733</td>\n",
       "      <td>1.280859</td>\n",
       "      <td>2.515148</td>\n",
       "      <td>1.299519</td>\n",
       "      <td>1.453432</td>\n",
       "      <td>0.474275</td>\n",
       "      <td>-1.396562</td>\n",
       "      <td>-0.647081</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064346</td>\n",
       "      <td>0.376469</td>\n",
       "      <td>0.277811</td>\n",
       "      <td>0.225676</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>0.540015</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.203476</td>\n",
       "      <td>0.346964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.581277</td>\n",
       "      <td>0.876188</td>\n",
       "      <td>1.042767</td>\n",
       "      <td>1.796120</td>\n",
       "      <td>2.541399</td>\n",
       "      <td>2.246653</td>\n",
       "      <td>1.500387</td>\n",
       "      <td>1.031521</td>\n",
       "      <td>0.382672</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070311</td>\n",
       "      <td>1.002770</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>0.916457</td>\n",
       "      <td>0.923975</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>0.656223</td>\n",
       "      <td>0.762357</td>\n",
       "      <td>0.501373</td>\n",
       "      <td>-0.333336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.689017</td>\n",
       "      <td>2.708703</td>\n",
       "      <td>2.008381</td>\n",
       "      <td>2.235800</td>\n",
       "      <td>1.516982</td>\n",
       "      <td>0.029916</td>\n",
       "      <td>-0.561346</td>\n",
       "      <td>-0.793702</td>\n",
       "      <td>-0.979371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137400</td>\n",
       "      <td>-0.136610</td>\n",
       "      <td>-0.072176</td>\n",
       "      <td>-0.082738</td>\n",
       "      <td>-0.138468</td>\n",
       "      <td>-0.120396</td>\n",
       "      <td>-0.089411</td>\n",
       "      <td>-0.243141</td>\n",
       "      <td>-0.119710</td>\n",
       "      <td>0.124042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.197677</td>\n",
       "      <td>0.455417</td>\n",
       "      <td>0.973110</td>\n",
       "      <td>1.935956</td>\n",
       "      <td>2.259463</td>\n",
       "      <td>1.741341</td>\n",
       "      <td>1.158296</td>\n",
       "      <td>0.418241</td>\n",
       "      <td>-0.071605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580230</td>\n",
       "      <td>0.482452</td>\n",
       "      <td>0.325569</td>\n",
       "      <td>0.247991</td>\n",
       "      <td>0.184127</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.241988</td>\n",
       "      <td>0.331451</td>\n",
       "      <td>-0.120006</td>\n",
       "      <td>0.042423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>1.038409</td>\n",
       "      <td>1.946421</td>\n",
       "      <td>2.705141</td>\n",
       "      <td>1.670706</td>\n",
       "      <td>-0.101167</td>\n",
       "      <td>-1.578876</td>\n",
       "      <td>-0.750906</td>\n",
       "      <td>0.175310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348410</td>\n",
       "      <td>0.324323</td>\n",
       "      <td>0.330489</td>\n",
       "      <td>0.111953</td>\n",
       "      <td>0.448948</td>\n",
       "      <td>0.567132</td>\n",
       "      <td>0.136757</td>\n",
       "      <td>0.444768</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.193378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>0.776054</td>\n",
       "      <td>2.181336</td>\n",
       "      <td>3.440325</td>\n",
       "      <td>2.168475</td>\n",
       "      <td>0.497315</td>\n",
       "      <td>-0.924284</td>\n",
       "      <td>-1.499227</td>\n",
       "      <td>-0.679328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253487</td>\n",
       "      <td>-0.058935</td>\n",
       "      <td>-0.130638</td>\n",
       "      <td>-0.347235</td>\n",
       "      <td>-0.177933</td>\n",
       "      <td>-0.060332</td>\n",
       "      <td>-0.347634</td>\n",
       "      <td>-0.447443</td>\n",
       "      <td>-0.066689</td>\n",
       "      <td>-0.178448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6         7  \\\n",
       "0   1.0  0.147647  0.804668  0.367771  0.243894  0.026614 -0.274402  0.096731   \n",
       "1  -1.0  0.316646  0.243199  0.370471  1.063738  1.678187  1.759558  1.697717   \n",
       "2  -1.0  1.168874  2.075901  1.760141  1.606446  1.949046  1.302842  0.459332   \n",
       "3   1.0  0.648658  0.752026  2.636231  3.455716  2.118157  0.520620 -0.188627   \n",
       "4   1.0  0.404733  1.280859  2.515148  1.299519  1.453432  0.474275 -1.396562   \n",
       "..  ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "94  1.0  0.581277  0.876188  1.042767  1.796120  2.541399  2.246653  1.500387   \n",
       "95 -1.0  2.689017  2.708703  2.008381  2.235800  1.516982  0.029916 -0.561346   \n",
       "96 -1.0  0.197677  0.455417  0.973110  1.935956  2.259463  1.741341  1.158296   \n",
       "97  1.0  0.179500  1.038409  1.946421  2.705141  1.670706 -0.101167 -1.578876   \n",
       "98  1.0  0.073124  0.776054  2.181336  3.440325  2.168475  0.497315 -0.924284   \n",
       "\n",
       "           8         9  ...        86        87        88        89        90  \\\n",
       "0  -0.747731 -1.609777  ... -0.770792 -0.533503 -0.400228  0.176084  1.111768   \n",
       "1   1.612159  1.168188  ...  0.771598  0.764229  0.610621  0.552900  0.566786   \n",
       "2   0.516412  0.852180  ...  0.186409  0.419006  0.723888  1.323947  2.136488   \n",
       "3   0.780818  0.933775  ... -0.077445 -0.097869 -0.136787 -0.340237 -0.089441   \n",
       "4  -0.647081  0.431945  ... -0.064346  0.376469  0.277811  0.225676  0.159091   \n",
       "..       ...       ...  ...       ...       ...       ...       ...       ...   \n",
       "94  1.031521  0.382672  ...  1.070311  1.002770  0.907869  0.916457  0.923975   \n",
       "95 -0.793702 -0.979371  ... -0.137400 -0.136610 -0.072176 -0.082738 -0.138468   \n",
       "96  0.418241 -0.071605  ...  0.580230  0.482452  0.325569  0.247991  0.184127   \n",
       "97 -0.750906  0.175310  ...  0.348410  0.324323  0.330489  0.111953  0.448948   \n",
       "98 -1.499227 -0.679328  ... -0.253487 -0.058935 -0.130638 -0.347235 -0.177933   \n",
       "\n",
       "          91        92        93        94        95  \n",
       "0   2.438428  2.734889  1.736054  0.036857 -1.265074  \n",
       "1   0.604002  0.777068  0.812345  0.748848  0.818042  \n",
       "2   1.746597  1.470220  1.893512  1.256949  0.800407  \n",
       "3  -0.080297 -0.192584 -0.304704 -0.454556  0.314590  \n",
       "4   0.408354  0.540015 -0.027791  0.203476  0.346964  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "94  0.767357  0.656223  0.762357  0.501373 -0.333336  \n",
       "95 -0.120396 -0.089411 -0.243141 -0.119710  0.124042  \n",
       "96  0.050358  0.241988  0.331451 -0.120006  0.042423  \n",
       "97  0.567132  0.136757  0.444768  0.151050  0.193378  \n",
       "98 -0.060332 -0.347634 -0.447443 -0.066689 -0.178448  \n",
       "\n",
       "[99 rows x 96 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('96',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True,axis=1)\n",
    "p=df['0']\n",
    "d=df.drop('0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      2      3      4      5      6      7      8      9      10     11     12     13     14     15     16     17     18     19     20     21     22     23     24     25     26     27     28     29     30     31     32     33     34     35     36     37     38     39     40     41     42     43     44     45     46     47     48     49     50     51     52     53     54     55     56     57     58     59     60     61     62     63     64     65     66     67     68     69     70     71     72     73     74     75     76     77     78     79     80     81     82     83     84     85     86     87     88     89     90     91     92     93     94     95     96   \n",
       "False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False  False    99\n",
       "dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Index(['0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(x,y=None,test_size=0.3):\n",
    "        if type(y) == None:\n",
    "            x=x.sample(frac=1)\n",
    "            return x.loc[:round(len(x)*test_size)],x.loc[round(len(x)*test_size):]\n",
    "        else :\n",
    "            n=[]\n",
    "            m=np.array(y)\n",
    "            m=np.unique(m)\n",
    "            a=pd.concat([x,y],axis=1)\n",
    "            for i in m:\n",
    "                n.append(a[a.iloc[:,-1] == i])\n",
    "            test,train = pd.DataFrame(),pd.DataFrame()\n",
    "            for i in n:\n",
    "                train=pd.concat([train,i.loc[round(len(x)*test_size):]])\n",
    "                test=pd.concat([test,i.loc[:round(len(x)*test_size)]])\n",
    "            train.sample(frac=1)\n",
    "            test.sample(frac=1)\n",
    "            p=a.columns[-1]\n",
    "            print(p,a.columns)\n",
    "            train_x,test_x,train_y,test_y = train.drop(p,axis=1),test.drop(p,axis=1),train[p],test[p]\n",
    "            return train_x.reset_index(),test_x.reset_index(),train_y.reset_index(),test_y.reset_index()\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "x,x1,x2,x3=train_test_split(p,d,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Index(['index', 'index', '0'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\Uni\\5thSem\\ML\\Lab\\Lab1\\Task1.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t1,y1,t2,y2 \u001b[39m=\u001b[39m train_test_split(x,x2,test_size\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m t3,y3,t4,y4 \u001b[39m=\u001b[39m train_test_split(x1,x3,test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;32me:\\Uni\\5thSem\\ML\\Lab\\Lab1\\Task1.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(x, y, test_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m p\u001b[39m=\u001b[39ma\u001b[39m.\u001b[39mcolumns[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(p,a\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train_x,test_x,train_y,test_y \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mdrop(p,axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),test\u001b[39m.\u001b[39mdrop(p,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),train[p],test[p]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_x\u001b[39m.\u001b[39mreset_index(),test_x\u001b[39m.\u001b[39mreset_index(),train_y\u001b[39m.\u001b[39mreset_index(),test_y\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4955\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4956\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4957\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4958\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4959\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4960\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4961\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4962\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4269\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "t1,y1,t2,y2 = train_test_split(x,x2,test_size=0.5)\n",
    "t3,y3,t4,y4 = train_test_split(x1,x3,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abbas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Uni\\5thSem\\ML\\Lab\\Lab1\\Task1.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([y1,y2,y3])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m clf\u001b[39m=\u001b[39m DecisionTreeClassifier()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m clf\u001b[39m=\u001b[39mclf\u001b[39m.\u001b[39;49mfit(train,test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Uni/5thSem/ML/Lab/Lab1/Task1.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pred\u001b[39m=\u001b[39mclf\u001b[39m.\u001b[39mpredict(t4)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    173\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[0;32m    900\u001b[0m             array,\n\u001b[0;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    904\u001b[0m         )\n\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train = pd.concat([t1,t2,t3])\n",
    "test = pd.concat([y1,y2,y3])\n",
    "clf= DecisionTreeClassifier()\n",
    "clf=clf.fit(train,test)\n",
    "pred=clf.predict(t4)\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c9ce850af2732a60cb8106a52d2de497310f0f3baa4f3f0f90c499266f24f48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
